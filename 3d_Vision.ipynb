{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "740a2e69",
   "metadata": {},
   "source": [
    "# 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b24127",
   "metadata": {},
   "source": [
    "How does a film/sensor actually capture a photo? The pinhole which is basically just a small aperature model make sure that pictures do not get blurry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090da06e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bc3c32d",
   "metadata": {},
   "source": [
    "![\"Screenshot from lectur about pinhole model\"](images/Screenshot%202025-12-09%20at%2019.26.35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617e2319",
   "metadata": {},
   "source": [
    "A major downside is that it doesn't gather much information because not much light is going through it. That is why modern camera's are using lenses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8710fa",
   "metadata": {},
   "source": [
    "![\"Screenshot of slide regarding lens camera\"](images/Screenshot%202025-12-09%20at%2019.32.36.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729707a3",
   "metadata": {},
   "source": [
    "## Projective geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e32150e",
   "metadata": {},
   "source": [
    "Projective geometry is used to develop an equation to map a 3d point to a 2d pixel. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4f7b9",
   "metadata": {},
   "source": [
    "![\"Screenshot of slide\"](images/Screenshot%202025-12-09%20at%2019.41.29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06bf47a",
   "metadata": {},
   "source": [
    "However this will not be done in cartesian coordinate system but in homogenous coordinates. Which is just basically:\n",
    "$$ (x,y) \\implies \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}  $$\n",
    "The homogenous coordinate does not care about the distance, it is also scale-invariant. This homegenous coordinate system makes it possible to do 3d matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d68958b",
   "metadata": {},
   "source": [
    "Converting to homogenous coordinate to cartesian is to always just add 1 as another dimension. It can be converted back by: \n",
    "$$ \\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix} => (x/w,y/w)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0479fab",
   "metadata": {},
   "source": [
    "as previously said it is invariant to scaling this is proved in the following: \n",
    "$$K\\begin{bmatrix}x\\\\y\\\\w\\\\\\end{bmatrix} = \\begin{bmatrix}kx\\\\ky\\\\kyw\\end{bmatrix}=>\\begin{bmatrix}\\frac{kx}{kw}\\\\\\frac{ky}{kw}\\end{bmatrix}=>\\begin{bmatrix}\\frac{x}{w}\\\\\\frac{y}{w}\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71a5529",
   "metadata": {},
   "source": [
    "So how do we map a 3d point to 2d map?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca85c02f",
   "metadata": {},
   "source": [
    "![\"Image for clarifying which point is which\"](images/Screenshot%202025-12-09%20at%2020.03.34.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf76ec3f",
   "metadata": {},
   "source": [
    "First we need to get rid of z which is possible by using the following principle:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be94e2b",
   "metadata": {},
   "source": [
    "![\"getting rid of z\"](images/Screenshot%202025-12-09%20at%2020.05.32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0498a4e5",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{x_s}{d} = \\frac{x_v}{z_v} \\quad \\text{and} \\quad \\frac{y_s}{d} = \\frac{y_v}{z_v}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6f50b0",
   "metadata": {},
   "source": [
    "These can be rearranged such that: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e15fb4",
   "metadata": {},
   "source": [
    "$$\n",
    "z_v \\frac{x_s}{1} = d \\frac{x_v}{1} \\quad \\text{and} \\quad z_v \\frac{y_s}{1} = d \\frac{y_v}{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f288ae65",
   "metadata": {},
   "source": [
    "this can also be converted into a matrix:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f6616",
   "metadata": {},
   "source": [
    "$$\n",
    "z_v \\begin{pmatrix}\n",
    "x_s \\\\\\\\\n",
    "y_s \\\\\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "d & 0 & 0 & 0 \\\\\\\\\n",
    "0 & d & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_v \\\\\\\\\n",
    "y_v \\\\\\\\\n",
    "z_v \\\\\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31a9bf1",
   "metadata": {},
   "source": [
    "d is our focal length also known as f which results in: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329b98d2",
   "metadata": {},
   "source": [
    "$$\n",
    "z_v \\begin{pmatrix}\n",
    "x_s \\\\\\\\\n",
    "y_s \\\\\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "f & 0 & 0 & 0 \\\\\\\\\n",
    "0 & f & 0 & 0 \\\\\\\\\n",
    "0 & 0 & 1 & 0\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x_v \\\\\\\\\n",
    "y_v \\\\\\\\\n",
    "z_v \\\\\\\\\n",
    "1\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3475450",
   "metadata": {},
   "source": [
    "However f is given in mm and we want to have the results in pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab5812",
   "metadata": {},
   "source": [
    "A new function F is introduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a821098",
   "metadata": {},
   "source": [
    "![\"Show the units of each metric\"](images/Screenshot%202025-12-09%20at%2020.16.41.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d8d64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e50bca5d",
   "metadata": {},
   "source": [
    "what must be included in F:\n",
    "1. Varying focal length\n",
    "2. Skew (not all pixels are rectangular)\n",
    "3. Radial distortions\n",
    "4. Principal Point offset\n",
    "5. Pixel size\n",
    "\n",
    "These are also called intrinsic carama parameters because those are varying per camera unit. Even between the same two camera's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ceafe",
   "metadata": {},
   "source": [
    "![\"Explanation of principal point\"](images/Screenshot%202025-12-11%20at%2010.05.58.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8331ab2c",
   "metadata": {},
   "source": [
    "## How do we calculate this F?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad317b6",
   "metadata": {},
   "source": [
    "1. Principal point offset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560119b8",
   "metadata": {},
   "source": [
    "![\"Replacement of u_0 and v_O\"](images/Screenshot%202025-12-11%20at%2011.20.22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf1d264",
   "metadata": {},
   "source": [
    "2. Non square pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab380b84",
   "metadata": {},
   "source": [
    "![\"Remove assumption: non-square pixels\"](images/Screenshot%202025-12-11%20at%2011.21.59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f45ff7",
   "metadata": {},
   "source": [
    "2. Skew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1165d9e",
   "metadata": {},
   "source": [
    "![\"Removing skew\"](images/Screenshot%202025-12-11%20at%2011.23.38.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5471f56b",
   "metadata": {},
   "source": [
    "W is not important, it is often also called f or z. The values for u and v should be rounded up to make it integers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ac3221",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c43fffa7",
   "metadata": {},
   "source": [
    "## Taking into account extrensic camera parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d8f68c",
   "metadata": {},
   "source": [
    "The extrensic camera matrix is the mathematical link between the real world coordinate system and the camera coordinate system. It therefore specificies two things:\n",
    "1. Camera position\n",
    "2. Camera orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f708b7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb7b052",
   "metadata": {},
   "source": [
    "![\"Screenshot of slide\"](images/Screenshot%202025-12-11%20at%2011.35.16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f2685e",
   "metadata": {},
   "source": [
    "For movement the updated matrix becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdada2d",
   "metadata": {},
   "source": [
    "![\"The camera is only being moved\"](images/Screenshot%202025-12-11%20at%2011.36.59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5ef50e",
   "metadata": {},
   "source": [
    " ![\"Counter-clockwise rotation around the coordinate axes\"](images/Screenshot%202025-12-11%20at%2011.40.54.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc3fa8a",
   "metadata": {},
   "source": [
    "These three need to be multiplied because all of them are acting on the x,y and z coordinate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a979489b",
   "metadata": {},
   "source": [
    "![\"Result of multiplication\"](images/Screenshot%202025-12-11%20at%2011.43.35.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cd9df9",
   "metadata": {},
   "source": [
    "The picture will not be asked on the exam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bc7f04",
   "metadata": {},
   "source": [
    "![\"Screenshot of slide\"](images/Screenshot%202025-12-11%20at%2011.46.07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b80e8a6",
   "metadata": {},
   "source": [
    "The end result will be:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81249bc7",
   "metadata": {},
   "source": [
    "![\"End result for extrensic camera parameters\"](images/Screenshot%202025-12-11%20at%2011.47.17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e21051",
   "metadata": {},
   "source": [
    "The r11 r12... are the ones provided in the end result which doesn't have to be studied for the exam. The left matrix is determining the qualtiy of the camera (intrinsic) while the right part is the extrensic part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e98e6d0",
   "metadata": {},
   "source": [
    "## Camera calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb687c2f",
   "metadata": {},
   "source": [
    "During calibration we are determining the intrincis and extrensic parameters. This is done by combining the intrinsic and extrensic into one matrix which is the projecteion matrix (3x4) which has 11 degrees of freedom (scaling invariance). This is the case becuase the right bottom value is always equal to 1. So we need to find 11 parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db42b768",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2285e11",
   "metadata": {},
   "source": [
    "How to do this (one of many examples):\n",
    "1. place the camera in a known position assume that this is the origin\n",
    "2. place a chess board in front of the camera with known distances between blocks\n",
    "3. take at least 6 (non-planar) points from this image and their corresponding real world coordinate, more is beneficial of course.\n",
    "4. Convert the following matrix equation to linear set of equations: ![\"screenshot of slide\"](images/Screenshot%202025-12-11%20at%2012.03.33.png)\n",
    "5. Rewrite this to: ![\"Screenshot of slide\"](images/Screenshot%202025-12-11%20at%2012.04.43.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bcb5ea",
   "metadata": {},
   "source": [
    "Real world case more than 6 points is used and the equation systems gets over-constrained and therefore it is solved using aLeast Squares Minimization. Not always results are found due to for example radial distortion and such things. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd7a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3bc1da",
   "metadata": {},
   "source": [
    "# 3D sensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207109cc",
   "metadata": {},
   "source": [
    "There are 2 types of sensors to determine the depth. Passive (indirect) which do not emit any acoustic signals such as stereo camera or a moving camera. There are also active (direct) sensors such as ToF, LiDARs, structured light sensors, light-field sensors which uses different focal lengths and such. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f91dd5",
   "metadata": {},
   "source": [
    "![\"The principles of structured light sensor\"](images/Screenshot%202025-12-11%20at%2012.31.40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14d414",
   "metadata": {},
   "source": [
    "This sensor is highly sensitive to sunlight due to IR coming from the sun. Multiple structured light sensors and time of flight sensors will give issues due to interference on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e13e68",
   "metadata": {},
   "source": [
    "The eyes use disparity. The following picture shows this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec6865",
   "metadata": {},
   "source": [
    " ![\"Disparity in eye\"](images/Screenshot%202025-12-11%20at%2012.39.55.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c934b868",
   "metadata": {},
   "source": [
    "In cameras the depth is also calculated from disparity. This is done by using feature matching with for example SIFT features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e5bd81",
   "metadata": {},
   "source": [
    " ![\"Disparity in camera\"](images/Screenshot%202025-12-11%20at%2012.42.42.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62c740f",
   "metadata": {},
   "source": [
    "![\"Screenshot of slide\"](images/Screenshot%202025-12-11%20at%2012.44.16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68da0ed",
   "metadata": {},
   "source": [
    "From disparity we can go quite easily to depth namely:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ea1ca1",
   "metadata": {},
   "source": [
    "::: {.callout-note title=\"Disparity to depth\"}\n",
    "The depth can be calculated using:\n",
    "$$\n",
    "z=\\frac{b*f}{p*d},\n",
    "$$\n",
    "**Where:**\n",
    "* $z$ is depth\n",
    "* $b$ is baseline\n",
    "* $f$ is focal length\n",
    "* $p$ is pixel width\n",
    "* $d$ is disparity\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319c183",
   "metadata": {},
   "source": [
    "Non prominent areas in a picture will not contain any features and therefore no features can be matched and no disparity can be calculated. Also if the matching is incorrect the depth will also be completely different. This is a major drawback. Therefore stereo cameras are not reliable. However a stereo camera can have unlimited range although this is still based on the baseline. So if you can use active sensor, however there a various reasons why you must use passive setup, in which using ir can be harmfull for the environment or to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99e9a28",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd197de",
   "metadata": {},
   "source": [
    "LiDaR uses different scanning mechanisms to make sure that it records in 2d instead of in only one direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a76bda",
   "metadata": {},
   "source": [
    "![\"Scanning mechanism LiDAR\"](images/Screenshot%202025-12-11%20at%2014.46.32.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f012ded4",
   "metadata": {},
   "source": [
    "4D imaging radar actually gives 4 dimensions:\n",
    "1. Distance\n",
    "2. Direction\n",
    "3. Height\n",
    "4. Object speed (based on the Doppler effect)\n",
    "5. (Actually also material)\n",
    "\n",
    "This radar is als GDPR approved. all other sensors cannot see through fog however 4d imaging radar can. This radar sends RF signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a914badd",
   "metadata": {},
   "source": [
    "## How to store the 3D data\n",
    "1. Voxel\n",
    "![\"Voxel grid\"](images/Screenshot%202025-12-11%20at%2015.04.26.png)\n",
    "2. Octree-based Model\n",
    "![\"Octree-based model\"](images/Screenshot%202025-12-11%20at%2015.07.36.png)\n",
    "3. Point cloud\n",
    "![\"Point cloud\"](images/Screenshot%202025-12-11%20at%2015.10.12.png)\n",
    "4. Mesh \n",
    "![\"Mesh\"](images/Screenshot%202025-12-11%20at%2015.11.48.png)\n",
    "5. Depth image (RGB-D)\n",
    "Just images each with a depth-value\n",
    "6. NeRF\n",
    "![\"NeRF\"](images/Screenshot%202025-12-11%20at%2015.13.34.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf02103",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da25219d",
   "metadata": {},
   "source": [
    "## NeRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5d2da1",
   "metadata": {},
   "source": [
    "The renderling loss is calculated using a rendered image from exactly the same viewing point as from where the image is taken. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e7744c",
   "metadata": {},
   "source": [
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65b3dc",
   "metadata": {},
   "source": [
    "## Gaussian splattering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96d194",
   "metadata": {},
   "source": [
    "### initialisation\n",
    "Uses Structure-from-Motion to initialize the 3d gaussians. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95359e7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fb0595d",
   "metadata": {},
   "source": [
    "## 3D Data capturing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c13104",
   "metadata": {},
   "source": [
    "To add multiple pictures together for a 3d model we need to know the *Sensor position and orientation*, timestaps would also be nice but not necessary. How can we actually find sensor position and orientation at every moment in time? SLAM (Simultaneous localication and mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b518df",
   "metadata": {},
   "source": [
    "### SLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc6ba72",
   "metadata": {},
   "source": [
    "SLAM was invented mostly for localizations of robots instead of 3d reconstruction. The main difficulty that it tackles is is that to make a 3d model we need to know our position however to determine our position we need a 3d map. There are many SLAM algorithms for various different sensors/cameras. Normally ROS2 works good for robotics. Many SLAMs use detection and tracking of landmarks/features in the images as a cornerstore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3c6e2d",
   "metadata": {},
   "source": [
    "![\"Main steps of SLAM\"](images/Screenshot%202025-12-18%20at%2009.20.28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5bd315",
   "metadata": {},
   "source": [
    "Of course the position is only an estimation there will always be an error. This error is alsoc called drift. This drift will become larger the more positions and frames there are added. This needs to be corrected because it can corrupt the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d6ddee",
   "metadata": {},
   "source": [
    "![\"Process for correcting pose\"](images/Screenshot%202025-12-18%20at%2010.09.55.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92c8bf4",
   "metadata": {},
   "source": [
    "This means that the model needs to be recalculated at every step. So that is computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a3d002",
   "metadata": {},
   "source": [
    "So a full slam procedure would look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c490ab",
   "metadata": {},
   "source": [
    "![\"Full SLAM loop\"](images/Screenshot%202025-12-18%20at%2010.18.21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54f4d2b",
   "metadata": {},
   "source": [
    "We first create a voxel grid, for this explanation a size of 256*256*256 is used. These are initialised to -1. The calculation of this point can be done using K again. This time the inverse of K needs to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343fb124",
   "metadata": {},
   "source": [
    "![\"Image from slide\"](images/Screenshot%202025-12-18%20at%2010.34.05.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e6c19",
   "metadata": {},
   "source": [
    "![\"Image of the slide\"](images/Screenshot%202025-12-18%20at%2010.35.04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bd61e7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "5ARE0_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
