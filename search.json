[
  {
    "objectID": "CNN.html",
    "href": "CNN.html",
    "title": "CNN",
    "section": "",
    "text": "The CNN succeeded because not every neuron in a layer is connected to all neurons in a different layer. Besides the hnadling was changed into a scale invariant method.The cnn network trains the values for the kernels. The input layer becomes the amount of pixels."
  },
  {
    "objectID": "CNN.html#general-information",
    "href": "CNN.html#general-information",
    "title": "CNN",
    "section": "General information",
    "text": "General information\n\n\n\n\n\n\nNotePerceptive field\n\n\n\nThe receptive field is the combination of pixels that, the specific neuron is using as input\n\n\n\n\n\nScreenshot of slide\n\n\nRepeating this turns into:\n\n\n\nScreenshot of slide\n\n\n\n\n\n\n\n\nTipKernel\n\n\n\nThese k*k weights basically provide a kernel and therefore we are training the kernels. The weights w become the parameters of the filters\n\n\nEach picture contains 3 colors so for every color there are three channels. Due to this the kernels are not 2x2 but most of the times it is 3x3.\nYou can apply different kernels in the same layer which also creates extra depth. This is happening in a different channel. This channel is also called a This depth is why we call it deep learning.\n\n\n\nScreenshot of Slide\n\n\nA pooling layer takes the outcomes of the convulotion layer and applies a kernel over this. In most cases this is a max-pool which selects the highest value. The following image shows an example for this.\n\n\n\nScreenshot of lecture slide of pooling layer\n\n\nThe stride is how many pixels it moves to the left. Stride 2 is therefore jumping over 1 pixel.\nThe low level features are learned in the ealrier layers. The higher level features are learned in later layers.\nA fully connected layer is applied to all of the result to get the final prediction."
  },
  {
    "objectID": "CNN.html#applying-techniques",
    "href": "CNN.html#applying-techniques",
    "title": "CNN",
    "section": "Applying techniques",
    "text": "Applying techniques\n\n1d convulotion\n1d convulotion is applied to the last convulotion layer such that the depth is reduced to 1 again.\n\n\n\nScreenshot of 1d convulotion\n\n\n\n\nHow to reduce overfitting?\n\nSimplify the model (reduce the amount of neurons and layers)\nPenalize high values for kernels by regularization\nDropout, randomly remove some (typically 50%) of the neurons in the network during each training iteration.\nBatch normalization, normalize (gaussian) the outcome of the activation function.\n\n\nBatch normalization\n\n\n\nScreenshot of batch normalization"
  },
  {
    "objectID": "CNN.html#practical-aspects",
    "href": "CNN.html#practical-aspects",
    "title": "CNN",
    "section": "Practical aspects",
    "text": "Practical aspects\n\n\n\n\n\n\nTiptransfer learning\n\n\n\nIn most cases it is actually better to use a pre-trained model eventhough this model is trained on a different item (must be images) because it already knows how to extract features. This is also called transfer learning.\n\n\nBatch gradient descent can lead to very slow training because it needs to save all the updates of each weight for each image in the dataset. Stochastic gradient descent can be quick however can lead to an osicilating learning curve. Mini-batch gradient descent is a sort of combination of both. These mini-batches should contain each class.\nTo evaluate global performance, and analyse the precision-recall trade-off.\n\n\n\n“Screenshot of slides”\n\n\nA ROC curve can also be used which has the same principles.\n\n\n\n“Screenshot of slides”"
  },
  {
    "objectID": "3d_Vision.html",
    "href": "3d_Vision.html",
    "title": "3D",
    "section": "",
    "text": "How does a film/sensor actually capture a photo? The pinhole which is basically just a small aperature model make sure that pictures do not get blurry.\nA major downside is that it doesn’t gather much information because not much light is going through it. That is why modern camera’s are using lenses"
  },
  {
    "objectID": "3d_Vision.html#projective-geometry",
    "href": "3d_Vision.html#projective-geometry",
    "title": "3D",
    "section": "Projective geometry",
    "text": "Projective geometry\nProjective geometry is used to develop an equation to map a 3d point to a 2d pixel.\n\n\n\n“Screenshot of slide”\n\n\nHowever this will not be done in cartesian coordinate system but in homogenous coordinates. Which is just basically: \\[ (x,y) \\implies \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix}  \\] The homogenous coordinate does not care about the distance, it is also scale-invariant. This homegenous coordinate system makes it possible to do 3d matrix multiplication\nConverting to homogenous coordinate to cartesian is to always just add 1 as another dimension. It can be converted back by: \\[ \\begin{bmatrix} x \\\\ y \\\\ w \\end{bmatrix} =&gt; (x/w,y/w)\\]\nas previously said it is invariant to scaling this is proved in the following: \\[K\\begin{bmatrix}x\\\\y\\\\w\\\\\\end{bmatrix} = \\begin{bmatrix}kx\\\\ky\\\\kyw\\end{bmatrix}=&gt;\\begin{bmatrix}\\frac{kx}{kw}\\\\\\frac{ky}{kw}\\end{bmatrix}=&gt;\\begin{bmatrix}\\frac{x}{w}\\\\\\frac{y}{w}\\end{bmatrix}\\]\nSo how do we map a 3d point to 2d map?\n\n\n\n“Image for clarifying which point is which”\n\n\nFirst we need to get rid of z which is possible by using the following principle:\n\n\n\n“getting rid of z”\n\n\n\\[\n\\frac{x_s}{d} = \\frac{x_v}{z_v} \\quad \\text{and} \\quad \\frac{y_s}{d} = \\frac{y_v}{z_v}\n\\]\nThese can be rearranged such that:\n\\[\nz_v \\frac{x_s}{1} = d \\frac{x_v}{1} \\quad \\text{and} \\quad z_v \\frac{y_s}{1} = d \\frac{y_v}{1}\n\\]\nthis can also be converted into a matrix:\n\\[\nz_v \\begin{pmatrix}\nx_s \\\\\\\\\ny_s \\\\\\\\\n1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nd & 0 & 0 & 0 \\\\\\\\\n0 & d & 0 & 0 \\\\\\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_v \\\\\\\\\ny_v \\\\\\\\\nz_v \\\\\\\\\n1\n\\end{pmatrix}\n\\]\nd is our focal length also known as f which results in:\n\\[\nz_v \\begin{pmatrix}\nx_s \\\\\\\\\ny_s \\\\\\\\\n1\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nf & 0 & 0 & 0 \\\\\\\\\n0 & f & 0 & 0 \\\\\\\\\n0 & 0 & 1 & 0\n\\end{pmatrix}\n\\begin{pmatrix}\nx_v \\\\\\\\\ny_v \\\\\\\\\nz_v \\\\\\\\\n1\n\\end{pmatrix}\n\\]\nHowever f is given in mm and we want to have the results in pixels.\nA new function F is introduced\n\n\n\n“Show the units of each metric”\n\n\nwhat must be included in F: 1. Varying focal length 2. Skew (not all pixels are rectangular) 3. Radial distortions 4. Principal Point offset 5. Pixel size\nThese are also called intrinsic carama parameters because those are varying per camera unit. Even between the same two camera’s\n\n\n\n“Explanation of principal point”"
  },
  {
    "objectID": "3d_Vision.html#how-do-we-calculate-this-f",
    "href": "3d_Vision.html#how-do-we-calculate-this-f",
    "title": "3D",
    "section": "How do we calculate this F?",
    "text": "How do we calculate this F?\n\nPrincipal point offset\n\n\n\n\n“Replacement of u_0 and v_O”\n\n\n\nNon square pixels\n\n\n\n\n“Remove assumption: non-square pixels”\n\n\n\nSkew\n\n\n\n\n“Removing skew”\n\n\nW is not important, it is often also called f or z. The values for u and v should be rounded up to make it integers"
  },
  {
    "objectID": "3d_Vision.html#taking-into-account-extrensic-camera-parameters",
    "href": "3d_Vision.html#taking-into-account-extrensic-camera-parameters",
    "title": "3D",
    "section": "Taking into account extrensic camera parameters",
    "text": "Taking into account extrensic camera parameters\nThe extrensic camera matrix is the mathematical link between the real world coordinate system and the camera coordinate system. It therefore specificies two things: 1. Camera position 2. Camera orientation\n\n\n\n“Screenshot of slide”\n\n\nFor movement the updated matrix becomes:\n\n\n\n“The camera is only being moved”\n\n\n\n\n\n“Counter-clockwise rotation around the coordinate axes”\n\n\nThese three need to be multiplied because all of them are acting on the x,y and z coordinate.\n\n\n\n“Result of multiplication”\n\n\nThe picture will not be asked on the exam.\n\n\n\n“Screenshot of slide”\n\n\nThe end result will be:\n\n\n\n“End result for extrensic camera parameters”\n\n\nThe r11 r12… are the ones provided in the end result which doesn’t have to be studied for the exam. The left matrix is determining the qualtiy of the camera (intrinsic) while the right part is the extrensic part."
  },
  {
    "objectID": "3d_Vision.html#camera-calibration",
    "href": "3d_Vision.html#camera-calibration",
    "title": "3D",
    "section": "Camera calibration",
    "text": "Camera calibration\nDuring calibration we are determining the intrincis and extrensic parameters. This is done by combining the intrinsic and extrensic into one matrix which is the projecteion matrix (3x4) which has 11 degrees of freedom (scaling invariance). This is the case becuase the right bottom value is always equal to 1. So we need to find 11 parameters\nHow to do this (one of many examples): 1. place the camera in a known position assume that this is the origin 2. place a chess board in front of the camera with known distances between blocks 3. take at least 6 (non-planar) points from this image and their corresponding real world coordinate, more is beneficial of course. 4. Convert the following matrix equation to linear set of equations:  5. Rewrite this to: \nReal world case more than 6 points is used and the equation systems gets over-constrained and therefore it is solved using aLeast Squares Minimization. Not always results are found due to for example radial distortion and such things."
  },
  {
    "objectID": "Sample.html",
    "href": "Sample.html",
    "title": "Style Test",
    "section": "",
    "text": "NoteBlue Note Test\n\n\n\nIf this box has a dark blue header and light blue body, your styles.css is working correctly.\n\n\n\n\n\n\n\n\nWarningOrange Warning Test\n\n\n\nIf this box has a dark orange header and light orange body, your custom warning style is working correctly."
  },
  {
    "objectID": "Sample.html#testing-custom-styles",
    "href": "Sample.html#testing-custom-styles",
    "title": "Style Test",
    "section": "",
    "text": "NoteBlue Note Test\n\n\n\nIf this box has a dark blue header and light blue body, your styles.css is working correctly.\n\n\n\n\n\n\n\n\nWarningOrange Warning Test\n\n\n\nIf this box has a dark orange header and light orange body, your custom warning style is working correctly."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "5LSH0 computer vision and ai",
    "section": "",
    "text": "Welcome to the lecture notes for computer vision and ai.\nThe table below lists all available lecture notes and automatically updates when you add new files to this repository."
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "5LSH0 computer vision and ai",
    "section": "",
    "text": "Welcome to the lecture notes for computer vision and ai.\nThe table below lists all available lecture notes and automatically updates when you add new files to this repository."
  },
  {
    "objectID": "index.html#lecture-notes",
    "href": "index.html#lecture-notes",
    "title": "5LSH0 computer vision and ai",
    "section": "Lecture Notes",
    "text": "Lecture Notes"
  }
]